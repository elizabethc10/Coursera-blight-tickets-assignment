{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This assignment was for my Machine Learning class on Coursera. The problem was coming up with probabilities that tickets for\n",
    "#blight violations in Detroit would be paid. At first I spent a lot of effort coming up with ways to get usable features\n",
    "#from different kinds of information, but then I found out that using just a few features would be enough.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def blight_model():\n",
    "    \n",
    "    #read test and training data\n",
    "    dfTrain = pd.read_csv('train.csv', encoding = 'ISO-8859-1')\n",
    "    dfTest = pd.read_csv('test.csv', encoding = 'ISO-8859-1')\n",
    "\n",
    "    #set indexes to ticket ID\n",
    "    dfTrain.set_index('ticket_id', inplace=True)\n",
    "    dfTest.set_index('ticket_id', inplace=True)\n",
    "\n",
    "    #dropping columns in training data that aren't in test data\n",
    "    cols = pd.DataFrame(dfTrain.columns).merge(pd.DataFrame(dfTest.columns), how='left', indicator=True)\n",
    "    colNames = cols[0]\n",
    "    dropCols = colNames[cols['_merge'] == 'left_only'].drop(colNames[colNames == 'compliance'].index[0])\n",
    "    dfTrain = dfTrain.drop(dropCols, axis=1)\n",
    "\n",
    "    #rename columns for convenience\n",
    "    colRename = {'violator_name': 'v_name', 'violation_street_number': 'v_str_num', 'violation_street_name': 'v_str_name',\n",
    "                 'mailing_address_str_number': 'm_str_num', 'mailing_address_str_name': 'm_str_name', 'violation_code': 'v_code',\n",
    "                 'violation_description': 'v_description'}\n",
    "    dfTrain.rename(columns=colRename, inplace=True)\n",
    "\n",
    "    #make strings lower case\n",
    "    strCols = ['disposition']\n",
    "    dfTrain[strCols] = dfTrain[strCols].apply(lambda a : a.astype(str).str.lower())\n",
    "\n",
    "    #Removing rows where judgment was not responsible or judgment still pending\n",
    "    notRespJudgments = ['not responsible by dismissal', 'not responsible by city dismissal', 'pending judgment',\n",
    "                        'not responsible by determination', 'set-aside (pending judgment)', 'responsible (fine waived) by deter']\n",
    "    notResp = dfTrain[dfTrain['disposition'].isin(notRespJudgments)]\n",
    "    dfTrain.drop(notResp.index, inplace=True)\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    keepCols = ['discount_amount', 'compliance', 'disposition']\n",
    "    dfTrain = dfTrain[keepCols]\n",
    "\n",
    "    #encode categorical data with get_dummies\n",
    "    catCols = ['disposition']\n",
    "    dfDum = pd.get_dummies(dfTrain[catCols], prefix=catCols, drop_first=True)\n",
    "    dfTrain = pd.concat([dfTrain, dfDum], axis=1).drop(catCols, axis=1)\n",
    "\n",
    "    #X and y\n",
    "    X = dfTrain.drop('compliance', axis=1)\n",
    "    y = dfTrain['compliance']\n",
    "\n",
    "    #MinMax scale\n",
    "    scaler = MinMaxScaler()\n",
    "    XMM = scaler.fit_transform(X)\n",
    "\n",
    "    #store ticket ID original order\n",
    "    TIDOrigOrder = dfTest.index\n",
    "\n",
    "    #all entries with Fine Waived in disposition should be in compliance\n",
    "    iFWaived = dfTest[dfTest['disposition'].str.contains('Fine Waived')].index\n",
    "    sFWaived = pd.Series(1, index=iFWaived)\n",
    "    dfTest.drop(iFWaived, inplace=True)\n",
    "\n",
    "    #preprocessing of Test data, almost same as for Training data\n",
    "\n",
    "    colRename = {'violator_name': 'v_name', 'violation_street_number': 'v_str_num', 'violation_street_name': 'v_str_name',\n",
    "                 'mailing_address_str_number': 'm_str_num', 'mailing_address_str_name': 'm_str_name', 'violation_code': 'v_code',\n",
    "                 'violation_description': 'v_description'}\n",
    "    dfTest.rename(columns=colRename, inplace=True)\n",
    "\n",
    "    strCols = ['disposition']\n",
    "    dfTest[strCols] = dfTest[strCols].apply(lambda a : a.astype(str).str.lower())\n",
    "\n",
    "    keepCols = ['discount_amount', 'disposition']\n",
    "    dfTest = dfTest[keepCols]\n",
    "\n",
    "    #fix a few disposition values\n",
    "    disp = dfTest['disposition']\n",
    "    disp = disp.str.replace('responsible - compl/adj by default', 'responsible by default')\n",
    "    disp = disp.str.replace('responsible by dismissal', 'responsible by default')\n",
    "    disp = disp.str.replace('responsible - compl/adj by determi', 'responsible by determination')\n",
    "    dfTest['disposition'] = disp\n",
    "\n",
    "    catCols = ['disposition']\n",
    "    dfDum = pd.get_dummies(dfTest[catCols], prefix=catCols, drop_first=True)\n",
    "    dfTest = pd.concat([dfTest, dfDum], axis=1).drop(catCols, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    testMM = scaler.fit_transform(dfTest)\n",
    "\n",
    "    clf = SGDClassifier(loss=\"log\")\n",
    "    clf.fit(XMM, y)\n",
    "\n",
    "    pr = clf.predict(testMM)\n",
    "    aPr = clf.predict_proba(testMM)\n",
    "    aPr = aPr[:,1]\n",
    "    sPr = pd.Series(aPr, index=dfTest.index)\n",
    "    sPr = sFWaived.append(sPr).reindex(TIDOrigOrder)\n",
    "\n",
    "    return sPr\n",
    "\n",
    "blight_model()\n",
    "\n",
    "\n",
    "\n",
    "#things I tried and didn't use:\n",
    "\n",
    "#dfTrain.info()\n",
    "#dfTrain.describe()\n",
    "#dfTrain.describe(include=['O'])\n",
    "\n",
    "#Ticket ID to Address, Address to Longitude/Latitude\n",
    "# TIDAddr = pd.read_csv('addresses.csv')\n",
    "# AddrLL = pd.read_csv('latlons.csv')\n",
    "\n",
    "# trainAddrs = dfTrain['ticket_id'].map(pd.Series(TIDAddr['address'], index=TIDAddr['ticket_id']))\n",
    "# dfTrain['lat'] = trainAddrs.map(pd.Series(AddrLL['lat'].values, index=AddrLL['address']))\n",
    "# dfTrain['lon'] = trainAddrs.map(pd.Series(AddrLL['lon'].values, index=AddrLL['address']))\n",
    "\n",
    "#display(dfTrain.head())\n",
    "#display(dfTest.head())\n",
    "\n",
    "#tried looking at periods between ticket issued date and hearing date\n",
    "# dfIH = dfTrain[['ticket_issued_date', 'hearing_date']]\n",
    "# dfIH = dfIH.applymap(lambda a : np.nan if pd.isna(a) else datetime.strptime(a, '%Y-%m-%d %H:%M:%S'))\n",
    "# (lambda a : a['hearing_date'] - a['ticket_issued_date'])(dfIH.dropna())\n",
    "#noticed some values are negative, decided not to use dates\n",
    "\n",
    "#removing punctuation from street names\n",
    "# makeT = ''.maketrans(\"\", \"\", string.punctuation)\n",
    "# strNames = dfTrain[['v_str_name', 'm_str_name']]\n",
    "# strNames = strNames.apply(lambda a : a.str.translate(makeT))\n",
    "#removing N, S, E, W from beginning of street names\n",
    "# strNames = strNames.applymap(lambda a : re.sub('^n\\s|^s\\s|^e\\s|^w\\s', '', str(a), 1))\n",
    "#adding column for whether name of violator street address matches name of mailing street address\n",
    "# dfTrain[['v_str_name', 'm_str_name']] = strNames\n",
    "# dfTrain['match_v_mail'] = dfTrain['v_str_name'].eq(dfTrain['m_str_name']).astype(int)\n",
    "\n",
    "#adding column for total number of properties cited for each violator\n",
    "#dfTrain['v_num_props'] = dfTrain['v_name'].map(dfTrain['v_name'].value_counts().to_dict())\n",
    "\n",
    "#fill missing state data with state missing\n",
    "# dfTrain['state'].replace('nan', np.NaN).fillna('state missing', inplace=True)\n",
    "#fill missing latitude and longitude data with mean\n",
    "# dfTrain['lat'].fillna(dfTrain['lat'].median(), inplace=True)\n",
    "# dfTrain['lon'].fillna(dfTrain['lon'].median(), inplace=True)\n",
    "\n",
    "#drop two agency_name outliers\n",
    "# ANNeighbor = dfTrain[dfTrain['agency_name'].str.contains('neighborhood city halls')].index.values\n",
    "# dfTrain.drop(ANNeighbor, inplace=True)\n",
    "\n",
    "#train-test split\n",
    "#train_x, test_x, train_y, test_y = train_test_split(XMM, y, random_state = 0)\n",
    "\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf.fit(train_x, train_y)\n",
    "# pr = clf.predict(test_x)\n",
    "# prProb = clf.predict_proba(test_x)\n",
    "# prProba = prProb[:,1]\n",
    "# print(roc_auc_score(test_y, prProba))\n",
    "\n",
    "# TIDAddr = pd.read_csv('addresses.csv')\n",
    "# AddrLL = pd.read_csv('latlons.csv')\n",
    "# testAddrs = dfTest['ticket_id'].map(pd.Series(TIDAddr['address'], index=TIDAddr['ticket_id']))\n",
    "# dfTest['lat'] = testAddrs.map(pd.Series(AddrLL['lat'].values, index=AddrLL['address']))\n",
    "# dfTest['lon'] = testAddrs.map(pd.Series(AddrLL['lon'].values, index=AddrLL['address']))\n",
    "\n",
    "# dfTest['v_num_props'] = dfTest['v_name'].map(dfTest['v_name'].value_counts().to_dict())\n",
    "\n",
    "# dfTest['state'].replace('nan', np.NaN).fillna('state missing', inplace=True)\n",
    "\n",
    "# dfTest['lat'].fillna(dfTest['lat'].median(), inplace=True)\n",
    "# dfTest['lon'].fillna(dfTest['lon'].median(), inplace=True)\n",
    "\n",
    "#add some columns for missing categorical values\n",
    "# X, dfTest = X.align(dfTest, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "# fmn = Nystroem()\n",
    "# XMMTr = fmn.fit_transform(XMM)\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf.fit(XMMTr, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
